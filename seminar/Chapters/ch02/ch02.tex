\chapter{LITERATURE REVIEW}
Several machine learning algorithms have been developed for performing the natural language processing task, where word embedding techniques play the significant role. Additi-onally, number of researches have been carried out in this topic, some of these are explained in the following section.\\
In paper \cite{bhoir2017comparative}, the author purposed a comparative analysis of different word embedding models namely continuous bag of words, skip gram, Glove (Global Vectors for word representation) and Hellinger- PCA. Additionally, models were compared on different parameters such as performance with respect to size of training data, basic over-view, and relation of context and target words, memory consumption etc. All in all, the author concluded that Glove is the best model as compared to other models as it is scalable to large corpus and worked well even with small corpus.\\
NPVec1, which consist of 25 SATA word embedding for Nepali language that derived from a large corpus using Glove, Word2Vec, fastText, and BERT is discussed in the paper  \cite{koirala2021npvec1}.  It also provided intrinsic and extrinstic evaluation of these embedding using well established metrices and methods. These models were trained using 279 million word tokens and were the largest embedding ever trained for Nepali language. Macro Precision, Recall and F1 metrics were used for the evaluation of the classification model. On average, the F1 scores for word embedding mod- els exceeded the baseline scores by a margin of 5 percent.\\
The author of the paper \cite{ajose2020performance}, conducted experiments on both classic and contextu-alised word embedding for the purpose of text classification. Moreover, to study the impact of word embedding on different dataset, the author selected four benchmarking classification datasets with varying average sample length for comparing both single-label and multi-label classification tasks. This study recommended that choosing CNN over BiLSTM for document classification dataset, where the context in sequence was not as indicative of class membership as sentence datasets. All in all, the author concluded that contextualized embedding was matched well with BiLSTM for SST-2-like classification datasets, while CNN collaborates well with classic embedding for document dataset like 20NewsGroup.\\
In the paper \cite{fano2019comparative}, the aim of the task was to evaluate system for early risk prediction on the internet, in particular to identify users suffering from eating disorders. In the controlled setting, the author evaluated the performance of three different word representation methods: random indexing, GloVe, and ELMo. The best model in term of F1 score turned out to be a model with GloVe vector as input to the text classifier and multi-layer perceptron.\\
In  paper \cite{ritu2018performance}, discussed the performance of three word embedding models namely, word2vec in tensorflow, word2vec from  Gensim package and FastText model. Additionally, dataset used in this research work contained 521391 unique words to produce the clusters and evaluated their performance in terms of accuracy and efficiency.\\
